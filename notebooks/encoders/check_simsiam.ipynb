{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888eb56b",
   "metadata": {},
   "source": [
    "## Training SimSiam and linear head\n",
    "\n",
    "Clone the following github: https://github.com/Reza-Safdari/SimSiam-91.9-top1-acc-on-CIFAR10\n",
    "\n",
    "### Training SimSiam:\n",
    "\n",
    "python main.py --exp_dir [Directory to store experiment models and results] --data_root /projects/leelab/data/image/cifar10/ --arch resnet18 --learning_rate 0.06 --epochs 800 --weight_decay 5e-4 --momentum 0.9 --batch_size 512 --gpu 7\n",
    "\n",
    "### Training linear head:\n",
    "\n",
    "python main_lincls.py --arch resnet18 --num_cls 10 --batch_size 256 --lr 30.0 --weight_decay 0.0 --pretrained [Best model (should be in exp_dir)] --gpu 7 /projects/leelab/data/image/cifar10/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5188f2",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a57150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from cl_explain.encoders.simsiam.resnet import ResNet\n",
    "\n",
    "device = 7\n",
    "\n",
    "data_path = '/projects/leelab/data/image/cifar10/'\n",
    "model_path = '/projects/leelab/cl-explainability/encoders/sim_siam/'\n",
    "\n",
    "def load_model(model_path, device):\n",
    "    model = ResNet(version=18, low_dim=10)\n",
    "    checkpoint = torch.load(model_path + 'resnet18.pth.tar')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.to(device)\n",
    "    return(model)\n",
    "\n",
    "model = load_model(model_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55846118",
   "metadata": {},
   "source": [
    "## Evaluate validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0235357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCHSIZE = 256\n",
    "NUMWORKERS = 20\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(int(32 * (8 / 7)), \n",
    "                      interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "test_set = datasets.CIFAR10(data_path, train=False, transform=transform_test)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=BATCHSIZE, shuffle=False, num_workers=NUMWORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify apply_eval_head is working as expected\n",
    "for img, label in test_loader:\n",
    "    break\n",
    "\n",
    "print(model(img.to(device), apply_eval_head=True).shape)\n",
    "print(model(img.to(device), apply_eval_head=False).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d00eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda(device)    \n",
    "\n",
    "def validate(test_loader, model, criterion, gpu=None, print_freq=500):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(test_loader):\n",
    "            if gpu is not None:\n",
    "                images = images.cuda(gpu, non_blocking=True)\n",
    "            target = target.cuda(gpu, non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images, apply_eval_head=True)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e22db",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(test_loader, model, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2537af7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl-explain-env",
   "language": "python",
   "name": "cl-explain-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
